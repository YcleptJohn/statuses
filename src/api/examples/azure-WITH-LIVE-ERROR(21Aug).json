{
  "items": [
    {
      "creator": "Ed Glas (Azure DevOps)",
      "title": "Availability Degradation",
      "link": "https://status.dev.azure.com/_event/201507499",
      "pubDate": "Wed, 19 Aug 2020 20:23:25 GMT",
      "content:encoded": "<p><strong>Final update</strong><br>Ed Glas, 8/20/2020 1:32:53 AM</p>\n<p>The issue is now fully mitigated. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. For customers who still hit this issue, sign out of Azure DevOps and sign in again in an incognito window. This will ensure the refresh token from AAD is up-to-date.</p>\n\n<hr>\n<p><strong>Update</strong><br>Ed Glas, 8/20/2020 1:23:30 AM</p>\n<p>We have fully mitigated the issue and will be working to prevent this class of incident from happening again. For customers who still hit this issue, sign out of Azure DevOps and sign in again in an incognito window. This will ensure the refresh token from AAD is up-to-date.</p>\n\n<hr>\n<p><strong>Update</strong><br>Ed Glas, 8/20/2020 12:42:11 AM</p>\n<p>We have identified the cause of the incident and are rolling out a mitigation.</p>\n\n<hr>\n<p><strong>Update</strong><br>Ed Glas, 8/19/2020 11:01:44 PM</p>\n<p>Our engineers are continuing to to understand and mitigate the issue.</p>\n\n<hr>\n<p><strong>Update</strong><br>Ed Glas, 8/19/2020 9:35:35 PM</p>\n<p>We are experiencing an issue with the authentication refresh token not being respected, which is forcing the signout. Interaction with the identity picker in particular can trigger this incident for a user.</p>\n<p>Users are asked to sign back in to Azure DevOps. Most users are successful at signing in, but some users are repeatedly prompted to sign in.</p>\n<p>We are actively working to find a mitigation for the incident.</p>\n\n<hr>\n<p><strong>Initial communication</strong><br>Adam Barr (VSNC), 8/19/2020 8:23:25 PM</p>\n<p>We've received reports of unexpected sign outs and token errors after signing in and are investigating.</p>\n\n",
      "content:encodedSnippet": "Final update\nEd Glas, 8/20/2020 1:32:53 AM\nThe issue is now fully mitigated. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. For customers who still hit this issue, sign out of Azure DevOps and sign in again in an incognito window. This will ensure the refresh token from AAD is up-to-date.\nUpdate\nEd Glas, 8/20/2020 1:23:30 AM\nWe have fully mitigated the issue and will be working to prevent this class of incident from happening again. For customers who still hit this issue, sign out of Azure DevOps and sign in again in an incognito window. This will ensure the refresh token from AAD is up-to-date.\nUpdate\nEd Glas, 8/20/2020 12:42:11 AM\nWe have identified the cause of the incident and are rolling out a mitigation.\nUpdate\nEd Glas, 8/19/2020 11:01:44 PM\nOur engineers are continuing to to understand and mitigate the issue.\nUpdate\nEd Glas, 8/19/2020 9:35:35 PM\nWe are experiencing an issue with the authentication refresh token not being respected, which is forcing the signout. Interaction with the identity picker in particular can trigger this incident for a user.\nUsers are asked to sign back in to Azure DevOps. Most users are successful at signing in, but some users are repeatedly prompted to sign in.\nWe are actively working to find a mitigation for the incident.\nInitial communication\nAdam Barr (VSNC), 8/19/2020 8:23:25 PM\nWe've received reports of unexpected sign outs and token errors after signing in and are investigating.",
      "dc:creator": "Ed Glas (Azure DevOps)",
      "content": "<p><strong>Final update</strong><br>Ed Glas, 8/20/2020 1:32:53 AM</p>\n<p>The issue is now fully mitigated. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. For customers who still hit this issue, sign out of Azure DevOps and sign in again in an incognito window. This will ensure the refresh token from AAD is up-to-date.</p>\n\n",
      "contentSnippet": "Final update\nEd Glas, 8/20/2020 1:32:53 AM\nThe issue is now fully mitigated. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. For customers who still hit this issue, sign out of Azure DevOps and sign in again in an incognito window. This will ensure the refresh token from AAD is up-to-date.",
      "guid": "https://status.dev.azure.com/_event/201507499",
      "isoDate": "2020-08-19T20:23:25.000Z"
    },
    {
      "creator": "Ed Glas (Azure DevOps)",
      "title": "Availability Degradation",
      "link": "https://status.dev.azure.com/_event/201485524",
      "pubDate": "Wed, 19 Aug 2020 14:58:40 GMT",
      "content:encoded": "<p><strong>Final update</strong><br>Ed Glas, 8/20/2020 12:38:33 AM</p>\n<p>We have identified the cause of the incident and are working on rolling out a mitigation.</p>\n\n<hr>\n<p><strong>Update</strong><br>Ed Glas, 8/20/2020 12:37:49 AM</p>\n<p>We have identified the cause of the incident and are working on rolling out a mitigation.</p>\n\n<hr>\n<p><strong>Update</strong><br>Ed Glas, 8/19/2020 3:39:43 PM</p>\n<p>This incident was limited to one of the 25 ATs running in one of our scale units in East US. It caused slow commands across Boards, Repos, and Pipelines, and a general slowness on the portal.</p>\n<p>The incident self-mitigated and we are investigating the root cause.</p>\n\n<hr>\n<p><strong>Initial communication</strong><br>mauro ottaviani, 8/19/2020 2:58:40 PM</p>\n<p>Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.</p>\n\n",
      "content:encodedSnippet": "Final update\nEd Glas, 8/20/2020 12:38:33 AM\nWe have identified the cause of the incident and are working on rolling out a mitigation.\nUpdate\nEd Glas, 8/20/2020 12:37:49 AM\nWe have identified the cause of the incident and are working on rolling out a mitigation.\nUpdate\nEd Glas, 8/19/2020 3:39:43 PM\nThis incident was limited to one of the 25 ATs running in one of our scale units in East US. It caused slow commands across Boards, Repos, and Pipelines, and a general slowness on the portal.\nThe incident self-mitigated and we are investigating the root cause.\nInitial communication\nmauro ottaviani, 8/19/2020 2:58:40 PM\nOur engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.",
      "dc:creator": "Ed Glas (Azure DevOps)",
      "content": "<p><strong>Final update</strong><br>Ed Glas, 8/20/2020 12:38:33 AM</p>\n<p>We have identified the cause of the incident and are working on rolling out a mitigation.</p>\n\n",
      "contentSnippet": "Final update\nEd Glas, 8/20/2020 12:38:33 AM\nWe have identified the cause of the incident and are working on rolling out a mitigation.",
      "guid": "https://status.dev.azure.com/_event/201485524",
      "isoDate": "2020-08-19T14:58:40.000Z"
    },
    {
      "creator": "Richa Kumar (Azure DevOps)",
      "title": "Search returning 500 errors in South Brazil region",
      "link": "https://status.dev.azure.com/_event/198327737",
      "pubDate": "Tue, 28 Jul 2020 07:16:07 GMT",
      "content:encoded": "<p><strong>Final update</strong><br>Richa Kumar, 7/29/2020 9:21:51 AM</p>\n<p>The issue is now mitigated for the recently active organizations and we are seeing a 100% success rate for search requests in this region. We are monitoring the success rate of requests from all organizations as the re-indexing continues to progress. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.</p>\n\n<hr>\n<p><strong>Update</strong><br>Richa Kumar, 7/29/2020 4:32:09 AM</p>\n<p>We are continuing to re-index the active organizations and currently see a 97% success rate for search requests in this region. We are monitoring the success rate of requests from all orgs as the re-indexing continues to progress.</p>\n\n<hr>\n<p><strong>Update</strong><br>Mihail Frintu, 7/29/2020 12:55:36 AM</p>\n<p>We are continuing to re-index the active organizations and currently see a 96% success rate for search requests in this region. We are monitoring the success rate of requests from all orgs as the re-indexing continues to progress.</p>\n\n<hr>\n<p><strong>Update</strong><br>Mihail Frintu, 7/28/2020 9:53:59 PM</p>\n<p>We are continuing to re-index the active organizations and currently see a 93% success rate for search requests in this region. We are monitoring the success rate of requests from all orgs as the re-indexing continues to progress.</p>\n\n<hr>\n<p><strong>Update</strong><br>Mihail Frintu, 7/28/2020 7:40:03 PM</p>\n<p>We are continuing to re-index the active organizations and currently see a 85% success rate for search requests in this region. We are monitoring the success rate of requests from all orgs as the re-indexing continues to progress.</p>\n\n<hr>\n<p><strong>Update</strong><br>Mihail Frintu, 7/28/2020 5:10:14 PM</p>\n<p>We've completed re-indexing of the largest recently active organizations and currently see a 73% success rate for search requests in this region.  We are monitoring the success rate of requests from all orgs as the re-indexing continues to progress.</p>\n\n<hr>\n<p><strong>Update</strong><br>Richa Kumar, 7/28/2020 2:03:40 PM</p>\n<p>Our engineers are continuing to work on the mitigation process to resolve the issue for all impacted accounts.</p>\n\n<hr>\n<p><strong>Update</strong><br>Richa Kumar, 7/28/2020 2:01:46 PM</p>\n<p>Our engineers are working to understand and mitigate the issue.</p>\n\n<hr>\n<p><strong>Update</strong><br>Richa Kumar, 7/28/2020 8:50:26 AM</p>\n<p>Our engineers have identified the root cause of the issue. The mitigation requires re-indexing for all impacted accounts. The re-indexing process has been triggered and impacted customers should start to see search behavior being restored once that process completes. We apologize for the inconvenience and will continue to provide updates on the resolution of the issue.</p>\n\n<hr>\n<p><strong>Initial communication</strong><br>Richa Kumar, 7/28/2020 7:16:07 AM</p>\n<p>Our engineers are currently investigating a service outage impacting Azure DevOps customers in the South Brazil region. Impacted customers will see error code 500 during Code, Work item, Wiki, or Package searches. The event is urgently being triaged and we will post an update just as soon as we know more.</p>\n\n",
      "content:encodedSnippet": "Final update\nRicha Kumar, 7/29/2020 9:21:51 AM\nThe issue is now mitigated for the recently active organizations and we are seeing a 100% success rate for search requests in this region. We are monitoring the success rate of requests from all organizations as the re-indexing continues to progress. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.\nUpdate\nRicha Kumar, 7/29/2020 4:32:09 AM\nWe are continuing to re-index the active organizations and currently see a 97% success rate for search requests in this region. We are monitoring the success rate of requests from all orgs as the re-indexing continues to progress.\nUpdate\nMihail Frintu, 7/29/2020 12:55:36 AM\nWe are continuing to re-index the active organizations and currently see a 96% success rate for search requests in this region. We are monitoring the success rate of requests from all orgs as the re-indexing continues to progress.\nUpdate\nMihail Frintu, 7/28/2020 9:53:59 PM\nWe are continuing to re-index the active organizations and currently see a 93% success rate for search requests in this region. We are monitoring the success rate of requests from all orgs as the re-indexing continues to progress.\nUpdate\nMihail Frintu, 7/28/2020 7:40:03 PM\nWe are continuing to re-index the active organizations and currently see a 85% success rate for search requests in this region. We are monitoring the success rate of requests from all orgs as the re-indexing continues to progress.\nUpdate\nMihail Frintu, 7/28/2020 5:10:14 PM\nWe've completed re-indexing of the largest recently active organizations and currently see a 73% success rate for search requests in this region.  We are monitoring the success rate of requests from all orgs as the re-indexing continues to progress.\nUpdate\nRicha Kumar, 7/28/2020 2:03:40 PM\nOur engineers are continuing to work on the mitigation process to resolve the issue for all impacted accounts.\nUpdate\nRicha Kumar, 7/28/2020 2:01:46 PM\nOur engineers are working to understand and mitigate the issue.\nUpdate\nRicha Kumar, 7/28/2020 8:50:26 AM\nOur engineers have identified the root cause of the issue. The mitigation requires re-indexing for all impacted accounts. The re-indexing process has been triggered and impacted customers should start to see search behavior being restored once that process completes. We apologize for the inconvenience and will continue to provide updates on the resolution of the issue.\nInitial communication\nRicha Kumar, 7/28/2020 7:16:07 AM\nOur engineers are currently investigating a service outage impacting Azure DevOps customers in the South Brazil region. Impacted customers will see error code 500 during Code, Work item, Wiki, or Package searches. The event is urgently being triaged and we will post an update just as soon as we know more.",
      "dc:creator": "Richa Kumar (Azure DevOps)",
      "content": "<p><strong>Final update</strong><br>Richa Kumar, 7/29/2020 9:21:51 AM</p>\n<p>The issue is now mitigated for the recently active organizations and we are seeing a 100% success rate for search requests in this region. We are monitoring the success rate of requests from all organizations as the re-indexing continues to progress. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.</p>\n\n",
      "contentSnippet": "Final update\nRicha Kumar, 7/29/2020 9:21:51 AM\nThe issue is now mitigated for the recently active organizations and we are seeing a 100% success rate for search requests in this region. We are monitoring the success rate of requests from all organizations as the re-indexing continues to progress. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.",
      "guid": "https://status.dev.azure.com/_event/198327737",
      "isoDate": "2020-07-28T07:16:07.000Z"
    },
    {
      "creator": "Adam Barr (VSNC) (Azure DevOps)",
      "title": "Slow Requests in West Europe",
      "link": "https://status.dev.azure.com/_event/198660078",
      "pubDate": "Thu, 30 Jul 2020 14:15:57 GMT",
      "content:encoded": "<p><strong>Final update</strong><br>Adam Barr (VSNC), 7/30/2020 2:50:53 PM</p>\n<p>An unhealthy compute node was added to a VM ScaleSet during normal auto-scale activity.  That node was serving very slow requests to some users.  We've taken that node out of rotation and are following up on why it was unhealthy.</p>\n\n<hr>\n<p><strong>Initial communication</strong><br>mauro ottaviani, 7/30/2020 2:15:57 PM</p>\n<p>Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.</p>\n\n",
      "content:encodedSnippet": "Final update\nAdam Barr (VSNC), 7/30/2020 2:50:53 PM\nAn unhealthy compute node was added to a VM ScaleSet during normal auto-scale activity.  That node was serving very slow requests to some users.  We've taken that node out of rotation and are following up on why it was unhealthy.\nInitial communication\nmauro ottaviani, 7/30/2020 2:15:57 PM\nOur engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.",
      "dc:creator": "Adam Barr (VSNC) (Azure DevOps)",
      "content": "<p><strong>Final update</strong><br>Adam Barr (VSNC), 7/30/2020 2:50:53 PM</p>\n<p>An unhealthy compute node was added to a VM ScaleSet during normal auto-scale activity.  That node was serving very slow requests to some users.  We've taken that node out of rotation and are following up on why it was unhealthy.</p>\n\n",
      "contentSnippet": "Final update\nAdam Barr (VSNC), 7/30/2020 2:50:53 PM\nAn unhealthy compute node was added to a VM ScaleSet during normal auto-scale activity.  That node was serving very slow requests to some users.  We've taken that node out of rotation and are following up on why it was unhealthy.",
      "guid": "https://status.dev.azure.com/_event/198660078",
      "isoDate": "2020-07-30T14:15:57.000Z"
    },
    {
      "creator": "Adam Barr (VSNC) (Azure DevOps)",
      "title": "Availability Degradation in East US 2 region",
      "link": "https://status.dev.azure.com/_event/201206833",
      "pubDate": "Mon, 17 Aug 2020 18:33:49 GMT",
      "content:encoded": "<p><strong>Final update</strong><br>Adam Barr (VSNC), 8/17/2020 8:07:05 PM</p>\n<p>The impact was caused by connectivity issues with between our services in that region.  With that mitigated, we see availability recover at 19:47 UTC.  We apologize for the impact this cause and will be investigating to learn how we can improve from this.</p>\n\n<hr>\n<p><strong>Update</strong><br>Adam Barr (VSNC), 8/17/2020 7:32:41 PM</p>\n<p>We're seeing intermittent, but widespread failed and slow requests in the East US 2 region and are investigating potential network issues with our partner teams.</p>\n\n<hr>\n<p><strong>Initial communication</strong><br>mauro ottaviani, 8/17/2020 6:33:49 PM</p>\n<p>Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.</p>\n\n",
      "content:encodedSnippet": "Final update\nAdam Barr (VSNC), 8/17/2020 8:07:05 PM\nThe impact was caused by connectivity issues with between our services in that region.  With that mitigated, we see availability recover at 19:47 UTC.  We apologize for the impact this cause and will be investigating to learn how we can improve from this.\nUpdate\nAdam Barr (VSNC), 8/17/2020 7:32:41 PM\nWe're seeing intermittent, but widespread failed and slow requests in the East US 2 region and are investigating potential network issues with our partner teams.\nInitial communication\nmauro ottaviani, 8/17/2020 6:33:49 PM\nOur engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.",
      "dc:creator": "Adam Barr (VSNC) (Azure DevOps)",
      "content": "<p><strong>Final update</strong><br>Adam Barr (VSNC), 8/17/2020 8:07:05 PM</p>\n<p>The impact was caused by connectivity issues with between our services in that region.  With that mitigated, we see availability recover at 19:47 UTC.  We apologize for the impact this cause and will be investigating to learn how we can improve from this.</p>\n\n",
      "contentSnippet": "Final update\nAdam Barr (VSNC), 8/17/2020 8:07:05 PM\nThe impact was caused by connectivity issues with between our services in that region.  With that mitigated, we see availability recover at 19:47 UTC.  We apologize for the impact this cause and will be investigating to learn how we can improve from this.",
      "guid": "https://status.dev.azure.com/_event/201206833",
      "isoDate": "2020-08-17T18:33:49.000Z"
    },
    {
      "creator": "Adam Barr (VSNC) (Azure DevOps)",
      "title": "Private pipeline agents deleting builds on network shares get stuck and must be recycled manually",
      "link": "https://status.dev.azure.com/_event/199763271",
      "pubDate": "Mon, 10 Aug 2020 18:37:40 GMT",
      "content:encoded": "<p><strong>Final update</strong><br>Adam Barr (VSNC), 8/11/2020 4:41:47 PM</p>\n<p>We've been monitoring to ensure we don't have any additional recurrence of agents getting stuck and have confirmed we don't see any new cases.</p>\n\n<hr>\n<p><strong>Update</strong><br>Adam Barr (VSNC), 8/10/2020 10:29:38 PM</p>\n<p>We now understand that the only agent pools impacted are those servicing build definitions that store artifacts on network shares.  Approximately 175 orgs are impacted.  At this point, we still believe the only mitigation for those stuck agents is a manual restart of the process, so we are leaving this service health advisory up for a little more time to ensure it is visible to anyone that hits this.</p>\n\n<hr>\n<p><strong>Initial communication</strong><br>Adam Barr (VSNC), 8/10/2020 6:37:40 PM</p>\n<p>Beginning as early as July 29, a bug in the handling of build deletion events caused private pipeline agents to get stuck in a processing state indefinitely.  The agent will look like it's running a job, but clicking into it will show a deleted build.  The change that introduced this bug was reverted August 10 at 17:45 UTC.  Our engineers are investigating whether there are any alternative approaches we can take to recover these agents from our side.</p>\n<p>At this time, though, any agent process stuck in this state must be manually restarted by the customer to recover.  Here is some guidance on restarting agents:<br />\n<a href=\"https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/v2-windows?view=azure-devops#how-do-i-restart-the-agent\" rel=\"nofollow\">Restart Windows agent</a><br />\n<a href=\"https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/v2-linux?view=azure-devops#how-do-i-restart-the-agent\" rel=\"nofollow\">Restart Linux agent</a><br />\n<a href=\"https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/v2-osx?view=azure-devops#how-do-i-restart-the-agent\" rel=\"nofollow\">Restart Mac agent</a></p>\n\n",
      "content:encodedSnippet": "Final update\nAdam Barr (VSNC), 8/11/2020 4:41:47 PM\nWe've been monitoring to ensure we don't have any additional recurrence of agents getting stuck and have confirmed we don't see any new cases.\nUpdate\nAdam Barr (VSNC), 8/10/2020 10:29:38 PM\nWe now understand that the only agent pools impacted are those servicing build definitions that store artifacts on network shares.  Approximately 175 orgs are impacted.  At this point, we still believe the only mitigation for those stuck agents is a manual restart of the process, so we are leaving this service health advisory up for a little more time to ensure it is visible to anyone that hits this.\nInitial communication\nAdam Barr (VSNC), 8/10/2020 6:37:40 PM\nBeginning as early as July 29, a bug in the handling of build deletion events caused private pipeline agents to get stuck in a processing state indefinitely.  The agent will look like it's running a job, but clicking into it will show a deleted build.  The change that introduced this bug was reverted August 10 at 17:45 UTC.  Our engineers are investigating whether there are any alternative approaches we can take to recover these agents from our side.\nAt this time, though, any agent process stuck in this state must be manually restarted by the customer to recover.  Here is some guidance on restarting agents:\nRestart Windows agent\nRestart Linux agent\nRestart Mac agent",
      "dc:creator": "Adam Barr (VSNC) (Azure DevOps)",
      "content": "<p><strong>Final update</strong><br>Adam Barr (VSNC), 8/11/2020 4:41:47 PM</p>\n<p>We've been monitoring to ensure we don't have any additional recurrence of agents getting stuck and have confirmed we don't see any new cases.</p>\n\n",
      "contentSnippet": "Final update\nAdam Barr (VSNC), 8/11/2020 4:41:47 PM\nWe've been monitoring to ensure we don't have any additional recurrence of agents getting stuck and have confirmed we don't see any new cases.",
      "guid": "https://status.dev.azure.com/_event/199763271",
      "isoDate": "2020-08-10T18:37:40.000Z"
    },
    {
      "creator": "Leah Antkiewicz (Azure DevOps)",
      "title": "YAML Pipeline cron schedules not triggering",
      "link": "https://status.dev.azure.com/_event/199232597",
      "pubDate": "Mon, 03 Aug 2020 21:53:47 GMT",
      "content:encoded": "<p><strong>Final update</strong><br>Leah Antkiewicz, 8/5/2020 2:09:21 PM</p>\n<p>All the schedule jobs have been updated that were effected by the production bug have been updated and the bug has been fixed. This issue is now resolved and there should be no re-occurrence going forward.</p>\n\n<hr>\n<p><strong>Update</strong><br>Leah Antkiewicz, 8/4/2020 11:29:45 PM</p>\n<p>The fix has been rolled out to all customers now but should take about ~6 hours to take effect for every customer. Each cron schedule is being updated which needs to be done over time so we don't impact the databases. An update will be made once all these jobs have been updated and the impact has been mitigated.</p>\n\n<hr>\n<p><strong>Update</strong><br>Leah Antkiewicz, 8/4/2020 5:43:40 PM</p>\n<p>We have begun to roll-out the fix for this issue. It has made it to the first 2 rings and has 2 more to go still which we will continue to push today.</p>\n<p>The mitigation steps are still valid if you need to be unblocked immediately.</p>\n\n<hr>\n<p><strong>Initial communication</strong><br>Adam Barr (VSNC), 8/3/2020 9:53:47 PM</p>\n<p>Pipeline cron schedules in YAML are not running for all customers currently. They appear to be scheduled in the UI but never actually run.</p>\n<p>A fix is being made currently to correct the bug but that most likely won't get deployed to all impacted rings until tomorrow.</p>\n<p>There is a workaround though: make a small, insignificant edit to your YAML file that contains the cron schedule.  This will refresh the cron schedules job and start the scheduled builds again</p>\n\n",
      "content:encodedSnippet": "Final update\nLeah Antkiewicz, 8/5/2020 2:09:21 PM\nAll the schedule jobs have been updated that were effected by the production bug have been updated and the bug has been fixed. This issue is now resolved and there should be no re-occurrence going forward.\nUpdate\nLeah Antkiewicz, 8/4/2020 11:29:45 PM\nThe fix has been rolled out to all customers now but should take about ~6 hours to take effect for every customer. Each cron schedule is being updated which needs to be done over time so we don't impact the databases. An update will be made once all these jobs have been updated and the impact has been mitigated.\nUpdate\nLeah Antkiewicz, 8/4/2020 5:43:40 PM\nWe have begun to roll-out the fix for this issue. It has made it to the first 2 rings and has 2 more to go still which we will continue to push today.\nThe mitigation steps are still valid if you need to be unblocked immediately.\nInitial communication\nAdam Barr (VSNC), 8/3/2020 9:53:47 PM\nPipeline cron schedules in YAML are not running for all customers currently. They appear to be scheduled in the UI but never actually run.\nA fix is being made currently to correct the bug but that most likely won't get deployed to all impacted rings until tomorrow.\nThere is a workaround though: make a small, insignificant edit to your YAML file that contains the cron schedule.  This will refresh the cron schedules job and start the scheduled builds again",
      "dc:creator": "Leah Antkiewicz (Azure DevOps)",
      "content": "<p><strong>Final update</strong><br>Leah Antkiewicz, 8/5/2020 2:09:21 PM</p>\n<p>All the schedule jobs have been updated that were effected by the production bug have been updated and the bug has been fixed. This issue is now resolved and there should be no re-occurrence going forward.</p>\n\n",
      "contentSnippet": "Final update\nLeah Antkiewicz, 8/5/2020 2:09:21 PM\nAll the schedule jobs have been updated that were effected by the production bug have been updated and the bug has been fixed. This issue is now resolved and there should be no re-occurrence going forward.",
      "guid": "https://status.dev.azure.com/_event/199232597",
      "isoDate": "2020-08-03T21:53:47.000Z"
    },
    {
      "creator": "Tommy Petty (Azure DevOps)",
      "title": "Availability Degradation",
      "link": "https://status.dev.azure.com/_event/199277296",
      "pubDate": "Mon, 03 Aug 2020 17:51:52 GMT",
      "content:encoded": "<p><strong>Final update</strong><br>Tommy Petty, 8/3/2020 6:21:02 PM</p>\n<p>The issue is now fully mitigated. The issue was related to a machine upgrade that failed. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.</p>\n\n<hr>\n<p><strong>Initial communication</strong><br>mauro ottaviani, 8/3/2020 5:51:52 PM</p>\n<p>Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.</p>\n\n",
      "content:encodedSnippet": "Final update\nTommy Petty, 8/3/2020 6:21:02 PM\nThe issue is now fully mitigated. The issue was related to a machine upgrade that failed. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.\nInitial communication\nmauro ottaviani, 8/3/2020 5:51:52 PM\nOur engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.",
      "dc:creator": "Tommy Petty (Azure DevOps)",
      "content": "<p><strong>Final update</strong><br>Tommy Petty, 8/3/2020 6:21:02 PM</p>\n<p>The issue is now fully mitigated. The issue was related to a machine upgrade that failed. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.</p>\n\n",
      "contentSnippet": "Final update\nTommy Petty, 8/3/2020 6:21:02 PM\nThe issue is now fully mitigated. The issue was related to a machine upgrade that failed. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.",
      "guid": "https://status.dev.azure.com/_event/199277296",
      "isoDate": "2020-08-03T17:51:52.000Z"
    },
    {
      "creator": "Leah Antkiewicz (Azure DevOps)",
      "title": "Pipelines failing for tag events from GitHub repositories ",
      "link": "https://status.dev.azure.com/_event/201800804",
      "pubDate": "Fri, 21 Aug 2020 17:01:57 GMT",
      "content:encoded": "<p><strong>Initial communication</strong><br>Leah Antkiewicz, 8/21/2020 5:01:57 PM</p>\n<p>For pipelines that are triggered by CI tag events, the pipelines are failing with the error message: &quot;Could not find a service connection for the repository&quot;. This is only for pipelines that use GitHub repositories as their source. A fix is being prepared for this now.</p>\n<p>As a workaround, the pipeline can be manually queued for that tag and the error will not occur.</p>\n\n",
      "content:encodedSnippet": "Initial communication\nLeah Antkiewicz, 8/21/2020 5:01:57 PM\nFor pipelines that are triggered by CI tag events, the pipelines are failing with the error message: \"Could not find a service connection for the repository\". This is only for pipelines that use GitHub repositories as their source. A fix is being prepared for this now.\nAs a workaround, the pipeline can be manually queued for that tag and the error will not occur.",
      "dc:creator": "Leah Antkiewicz (Azure DevOps)",
      "content": "<p><strong>Initial communication</strong><br>Leah Antkiewicz, 8/21/2020 5:01:57 PM</p>\n<p>For pipelines that are triggered by CI tag events, the pipelines are failing with the error message: &quot;Could not find a service connection for the repository&quot;. This is only for pipelines that use GitHub repositories as their source. A fix is being prepared for this now.</p>\n<p>As a workaround, the pipeline can be manually queued for that tag and the error will not occur.</p>\n\n",
      "contentSnippet": "Initial communication\nLeah Antkiewicz, 8/21/2020 5:01:57 PM\nFor pipelines that are triggered by CI tag events, the pipelines are failing with the error message: \"Could not find a service connection for the repository\". This is only for pipelines that use GitHub repositories as their source. A fix is being prepared for this now.\nAs a workaround, the pipeline can be manually queued for that tag and the error will not occur.",
      "guid": "https://status.dev.azure.com/_event/201800804",
      "isoDate": "2020-08-21T17:01:57.000Z"
    },
    {
      "creator": "Mihail Frintu (Azure DevOps)",
      "title": "Availability Degradation",
      "link": "https://status.dev.azure.com/_event/200818344",
      "pubDate": "Fri, 14 Aug 2020 15:07:36 GMT",
      "content:encoded": "<p><strong>Final update</strong><br>Mihail Frintu, 8/16/2020 11:30:18 PM</p>\n<p>The transient connectivity issue is not present anymore.</p>\n\n<hr>\n<p><strong>Update</strong><br>Mihail Frintu, 8/14/2020 4:27:26 PM</p>\n<p>Our engineers are monitoring and are not seeing any impact at this time. There was a transient issue that impacted a small number of users of Release Management features for a 10 minute window between 14:50-15:00 UTC in the Central US region.</p>\n\n<hr>\n<p><strong>Initial communication</strong><br>mauro ottaviani, 8/14/2020 3:07:36 PM</p>\n<p>Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.</p>\n\n",
      "content:encodedSnippet": "Final update\nMihail Frintu, 8/16/2020 11:30:18 PM\nThe transient connectivity issue is not present anymore.\nUpdate\nMihail Frintu, 8/14/2020 4:27:26 PM\nOur engineers are monitoring and are not seeing any impact at this time. There was a transient issue that impacted a small number of users of Release Management features for a 10 minute window between 14:50-15:00 UTC in the Central US region.\nInitial communication\nmauro ottaviani, 8/14/2020 3:07:36 PM\nOur engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.",
      "dc:creator": "Mihail Frintu (Azure DevOps)",
      "content": "<p><strong>Final update</strong><br>Mihail Frintu, 8/16/2020 11:30:18 PM</p>\n<p>The transient connectivity issue is not present anymore.</p>\n\n",
      "contentSnippet": "Final update\nMihail Frintu, 8/16/2020 11:30:18 PM\nThe transient connectivity issue is not present anymore.",
      "guid": "https://status.dev.azure.com/_event/200818344",
      "isoDate": "2020-08-14T15:07:36.000Z"
    }
  ],
  "feedUrl": "https://status.dev.azure.com/_rss",
  "title": "Azure DevOps - Recent Events",
  "description": "Recent events affecting availability of Azure DevOps Services.",
  "pubDate": "Wed, 19 Aug 2020 20:23:25 GMT",
  "link": "https://status.dev.azure.com/",
  "language": "en-US",
  "lastBuildDate": "Wed, 19 Aug 2020 20:23:25 GMT"
}